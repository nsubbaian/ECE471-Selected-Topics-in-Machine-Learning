# ECE471-Selected-Topics-in-Machine-Learning
Professor Curro's Website: http://ee.cooper.edu/~curro/cgml/

# Submissions
Four Assignments - respective folder contains assignment and submission
Midterm Project - reimplement an existing research paper
Final Project - improve on existing research

# Weekly Reading/Discussion based on the following papers:
### Week 1:
* Automatic Differentiation in Machine Learning: a Survey, https://arxiv.org/abs/1502.05767 
* Stochastic Gradient Descent Tricks, https://arxiv.org/abs/1502.05767v4
### Week 2:
* ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION, https://arxiv.org/abs/1412.6980
* Why Momentum Really Works, https://distill.pub/2017/momentum/
### Week 3:
* Going deeper with convolutions, https://arxiv.org/abs/1409.4842
* Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, https://arxiv.org/abs/1502.01852
* Identity Mappings in Deep Residual Networks, https://arxiv.org/pdf/1603.05027.pdf
### Week 4:
* Understanding intermediate layers using linear classifier probes, https://arxiv.org/abs/1610.01644v3
* REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDENT OUTPUT DISTRIBUTIONS, https://arxiv.org/abs/1701.06548
* Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models, https://arxiv.org/abs/1702.03275v2
### Week 5:
* SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE, https://arxiv.org/abs/1602.07360
* Densely Connected Convolutional Networks, https://arxiv.org/abs/1608.06993
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, https://arxiv.org/abs/1704.04861
### Week 6:
* Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, https://arxiv.org/abs/1703.03400
* UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION, https://arxiv.org/abs/1611.03530
* METRIC LEARNING WITH ADAPTIVE DENSITY DISCRIMINATION, https://arxiv.org/abs/1511.05939
### Week 7:
* Learning with Random Learning Rates, https://arxiv.org/abs/1810.01322
* DON’T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE, https://arxiv.org/abs/1711.00489
* A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY, https://arxiv.org/abs/1803.09820
### Week 8:
* Improved Training of Wasserstein GANs, https://arxiv.org/abs/1704.00028
* PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION, https://arxiv.org/abs/1710.10196
* Self-Attention Generative Adversarial Networks, https://arxiv.org/abs/1805.08318v1
* LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS, https://arxiv.org/abs/1809.11096
* A Style-Based Generator Architecture for Generative Adversarial Networks, https://arxiv.org/abs/1812.04948
### Week 9:
* A Neural Algorithm of Artistic Style, https://arxiv.org/abs/1508.06576
* A LEARNED REPRESENTATION FOR ARTISTIC STYLE, https://arxiv.org/abs/1610.07629
* Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization, https://arxiv.org/pdf/1703.06868.pdf
* A Flexible Convolutional Solver with Application to Photorealistic Style Transfer, https://arxiv.org/abs/1806.05285
* Learning Linear Transformations for Fast Arbitrary Style Transfer, https://arxiv.org/abs/1808.04537
### Week 10:
* A Brief Survey of Deep Reinforcement Learning, https://arxiv.org/abs/1708.05866v2 
### Week 11:
* WAVENET: A GENERATIVE MODEL FOR RAW AUDIO, https://arxiv.org/abs/1609.03499
* Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders, https://arxiv.org/abs/1704.01279
* HIERARCHICAL GENERATIVE MODELING FOR CONTROLLABLE SPEECH SYNTHESIS, https://arxiv.org/abs/1810.07217
* FFTNET: A REAL-TIME SPEAKER-DEPENDENT NEURAL VOCODER, https://gfx.cs.princeton.edu/pubs/Jin_2018_FAR/fftnet-jin2018.pdf
* WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS, https://arxiv.org/abs/1811.00002v1
### Week 12:
* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, https://arxiv.org/abs/1905.11946
* Self-training with Noisy Student improves ImageNet classification, https://arxiv.org/abs/1911.04252
* UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING, https://arxiv.org/abs/1904.12848v4
* MixMatch: A Holistic Approach to Semi-Supervised Learning, https://arxiv.org/abs/1905.02249

